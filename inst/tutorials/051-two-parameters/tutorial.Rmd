---
title: Two Parameters
author: David Kane and Mihir Kaushal
tutorial:
  id: two-parameters
output:
  learnr::tutorial:
    progressive: yes
    allow_skip: yes
runtime: shiny_prerendered
description: 'Chapter 5 Tutorial: Two Parameters'
---

```{r setup, include = FALSE}
library(learnr)
library(tutorial.helpers)
library(gt)

library(tidyverse)
library(primer.data)
library(skimr)
library(brms)
library(tidybayes)
library(gtsummary)

knitr::opts_chunk$set(echo = FALSE)
options(tutorial.exercise.timelimit = 600, 
        tutorial.storage = "local") 

ch5 <- nhanes |>
  filter(sex == "Male", age >= 18) |>
  select(height) |>
  drop_na()

# fit_male_height <- brm(formula = height ~ 1,
#              data = ch5,
#              family = gaussian(),
#              silent = 2,
#              refresh = 0,
#              seed = 12)
# 
# write_rds(fit_male_height, "data/fit_male_height.rds")

fit_male_height <- read_rds("data/fit_male_height.rds")

ndata <- tibble(.rows = 1)

```

```{r copy-code-chunk, child = system.file("child_documents/copy_button.Rmd", package = "tutorial.helpers")}
```

```{r info-section, child = system.file("child_documents/info_section.Rmd", package = "tutorial.helpers")}
```

## Introduction
### 

This tutorial covers [Chapter 5: Two Parameters](https://ppbds.github.io/primer/two-parameters.html) of [*Preceptor’s Primer for Bayesian Data Science: Using the Cardinal Virtues for Inference*](https://ppbds.github.io/primer/) by [David Kane](https://davidkane.info/).

In this chapter, we have *two* unknown parameters: the mean $\mu$ height in the US and the standard deviation, $\sigma$, of the normally distributed error term.

The reason for making models is not, primarily, that making models is fun --- although it is! The reason is that the world confronts us. Make decisions we must. We must decide between options X or Y. We must choose from actions A, B and C. Confronted by a choice, we need to make a model of the world to help us choose wisely.

In the real world, data scientists usually do not start with a specific question. They first have a general question or idea which they want to learn more about. In our case, this general question is: 

* What is the average height of men?

To answer our general question, we would need to create a model of height. In order to make progress, we need to drill down to a more specific question. Specifics help you to fix ideas as you start to work on a project. Just because you start looking for this number does not mean that we can’t consider other questions. 

## The Question
### 

*A prudent question is one half of wisdom.* - Francis Bacon

<!-- Both a causal effect and a prediction are much fuzzier notions than you might think because their are so many, depending on AGGREGATION. -->
 
<!-- If you don't care what Joe would have done in a counter-factual world in which we got a different treatment, if all you care about is predicting what Joe does *given the treatment he received*, then you just need a predictive model. -->

### Exercise 1

Load **tidyverse**.

```{r the-question-1, exercise = TRUE}

```

```{r the-question-1-hint-1, eval = FALSE}
library(tidyverse)
```

```{r the-question-1-test, include = FALSE}
library(tidyverse)
```

### 

We will be using the data set about US people's body shape and related measurements from the US National Health and Nutrition Examiniation Survey (NHANES) for the 2009-2010 and 2011-2012. See this [link](https://www.cdc.gov/nchs/nhanes/index.htm?CDC_AA_refVal=https%3A%2F%2Fwww.cdc.gov%2Fnchs%2Fnhanes.htm) for further details about NHANES.

### Exercise 2

Load the **primer.data** package.

```{r the-question-2, exercise = TRUE}

```

```{r the-question-2-hint-1, eval = FALSE}
library(primer.data)
```

```{r the-question-2-test, include = FALSE}
library(primer.data)
```

### 

A version of the data from the National Health and Nutrition Examination Survey is available in the `nhanes` tibble. This tibble consists `r scales::comma(nrow(nhanes))` rows and `r scales::comma(ncol(nhanes))` columns.

### Exercise 3

After loading **primer.data** in your Console, type `?nhanes` in the Console, and paste in the Description below.

```{r the-question-3}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

In order to get a demographically diverse range of responses, minority groups were surveyed more heavily. However, this skewed the survey results to have an inaccurate demographic makeup. To address this, some observations from more common groups are resampled in the data.


### Exercise 4

Height is the broad topic of this tutorial. Given that topic, which variable in `nhanes` should we use as our outcome variable? 

```{r the-question-4}
question_text(NULL,
	message = "We will be using `height` as an outcome variable.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 2)
```

### 

`height` is measured in centimeters.

```{r}
nhanes |> 
  select(height) |> 
  drop_na() |> 
  ggplot(aes(height)) +
    geom_histogram(bins = 120) +
    labs(title = "Height from NHANES",
         subtitle = "NHANES includes children and adults",
         x = "Height (cm)",
         y = "Number")
```

### Exercise 5

Let's imagine a brand new variable which **does not exists** in the data. This variable should be binary, meaning that it only takes on one of two values. It should also, at least in theory, by manipulable. In other words, if the value of the variable is "X," or whatever, then it generates one potential outcome and if it is "Y," or whatever, it generates another potential outcome.

Describe this variable and how might we manipulate its value?

```{r the-question-5}
question_text(NULL,
	message = "Imagine a variable called `vitamin`. We can manipulate the value of `vitamin`, at least in theory, by providing people in the treatment group with vitamins throughout their childhood while not giving vitamins to children in the control group.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

In this case, `vitamin` is a binary variable for which `1` means that the individual ate vitamins growing up and `0` means they did not. And obviously, there is no right or wrong answer as long as the variable you suggest can be manipulated, at least in theory. But, going forward, we will use the variable `vitamin`. 

<!-- XX: This question and the ones that follow are tricky. We are trying to drive home some related points. First, any data set can be used to construct a causal model as long as there is at least one covariate that we can, at least in theory, manipulate. It does not matter whether or not anyone did, in fact, manipulate it. Second, any data set can be used to construct a predictive model. Third, the same data set can be used to create, separately, lots and lots of different models, both causal and predictive. We can just use different outcome variables and/or specify different treatment variables. All of this stuff is a *conceptual* framework we apply to the data. It is never inherent in the data itself.-->

<!-- XX: Explain that there is no right answer and that the student's response was probably fine. But, going forward, the student should use the variable we have selected. That variable should be named in future questions so that this is clear to the student. -->

### Exercise 6

Given our choice of treatment variable `vitamin`, how many potential outcomes are there for each individual? Explain why.

```{r the-question-6}
question_text(NULL,
	message = "There are 2 potential outcomes because the treatment variable `vitamin` takes on 2 posible values: take vitamin versus no vitamin.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Any data set can be used to construct a predictive model. 

The same data set can be used to create, separately, lots and lots of different models, both causal and predictive. We can just use different outcome variables and/or specify different treatment variables. All of this stuff is a *conceptual* framework we apply to the data. It is never inherent in the data itself.

### Exercise 7

Write a sentence which speculates as to value of the XX different potential outcomes which we might observe in `height` for each individual when we change the value of the treatment variable `vitamin`.

```{r the-question-7}
question_text(NULL,
	message = "If we give an individual vitamin as additional supplements, his height would be higher compared to that of his when not taking any vitamin.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The point of the Rubin Causal Models is that the definition of a causal effect is the difference between potential outcomes. So, there must be two (or more) potential outcomes for any causal model to make sense. This is simplest to discuss when the treatment only has two different values, thereby generating only two potential outcomes. But, if the treatment variable is continuous, (like income) then there are lots and lots of potential outcomes, one for each possible value of the treatment variable. 

### Exercise 8

Write a few sentences which specify two different values for the treatment variable, for a single unit, and then guesses at the potential outcomes which would result, and then calculates the causal effect for that unit given those guesses.

```{r the-question-8}
question_text(NULL,
	message = "For a given individual, assume that the value of the treatment variables might be `take vitamin` or `no vitamin`. If the individual gets `take vitamin`, then `height` would be 175. If the individual gets `no vitamin`, then `height` would be 173. The causal effect on the outcome of a treatment of `vitamin` versus `no vitamin` is 175 - 173 --- i.e., the difference between two potential outcomes --- which equals 2, which is the causal effect.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The the definition of a causal effect as the difference between two potential outcomes. Of course, you can't just say that the causal effect is 2. The exact value depends on which potential outcome comes first in the subtraction and which second. There is, perhaps, a default sense in which the causal effect is defined as treatment minus control. 

Any causal connection means exploring the *within row* different between two potential outcomes. We don't need to look at any other rows to have that conversation.

### Exercise 9

Let's consider a *predictive* model. Which variable in `nhanes` do you think might have an important connection to `height`? (If you don't see a reasonable variable in the data, you can just name a variable which *might have been* included in the data.)

```{r the-question-9}
question_text(NULL,
	message = "`sex` is a potential variable that may relate to `height`.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 2)
```

### 

The key point is that, with a predictive model, there is only one outcome for each individual unit. There are not two potential outcomes because we are not considering any of the covariates to be treatment variables. We assuming that all covariates are "fixed." 

### Exercise 10

Write a few sentences which specify two different groups of individuals with different values for `sex`. Explain that the average value of the outcome variable might differ between these two groups.

```{r the-question-10}
question_text(NULL,
	message = "XX: Some individuals might have a value for `sex` of `Male`. Others might have a value of `Female`. Those two groups will, on average, have different values for the outcome variable.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

In predictive models, do not use words like "cause," "influence," "impact," or anything else which suggests causation. The best phrasing is in terms of "differences" between groups of units with different values for the covariate of interest.

### Exercise 11

Write a predictive question which involves the outcome variable `height`. 

```{r the-question-11}
question_text(NULL,
	message = "What is the average height of adult men?",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

You can only use causal language --- like "affect," "influence," "be associated with," "cause,", "causal effect," et cetera --- in your question if you are creating a causal model, one with a treatment variable which you might, at least in theory, manipulate and with at least two potential outcomes.

With a predictive model, your question should focus on a comparison between different rows, or groups of rows, in the Preceptor Table.

### Exercise 12

What is a Quantity of Interest which might help us to explore the answer to our question?


```{r the-question-12}
question_text(NULL,
	message = "The Quantity of Interest for this question is the *expected* value of `height` for Male.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Our Quantity of Interest might appear too specific, too narrow to capture the full complexity of the topic. There are many, many numbers which we are interested in, many numbers that we want to know. But we don't need to list them all here! We just need to choose one of them since our goal is just to have a specific number which helps to guide us in the creation of the Preceptor Table and, then, the model.

## Wisdom
### 

*The only true wisdom is in knowing you know nothing.* - Socrates

### Exercise 1

In your own words, describe the key components of Wisdom for working on a data science problem.

```{r wisdom-1}
question_text(NULL,
	message = "Wisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of validity, as to whether or not we can (reasonably!) assume that the two come from the same population.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Wisdom begins with the Preceptor Table. What data would we, ideally, require to answer our questions? We then explore the data that we actually have. We apply the concept of validity to ensure that the data we want and the data we have are similar enough to allow the latter to inform us about the former.

### Exercise 2

Define a Preceptor Table.

```{r wisdom-2}
question_text(NULL,
	message = "A Preceptor Table is the smallest possible table of data with rows and columns such that, if there is no missing data, it is easy to calculate the quantities of interest.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The Preceptor Table does not include all the covariates which you will eventually including in your model. It only includes covariates which you need to answer your question.

### Exercise 3

Describe the key components of Preceptor Tables in general, without worrying about this specific problem. Use words like "units," "outcomes," and "covariates."

```{r wisdom-3}
question_text(NULL,
	message = "The rows of the Preceptor Table are the units. The outcome is at least one of the columns. If the problem is causal, there will be at least two (potential) outcome columns. The other columns are covariates. If the problem is causal, at least one of the covariates will be a treatment.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

<!-- AC: The sentence below sounds a bit clunky. I tried rewording it to fit it a bit better, however it may warrant a complete rewording.  -->

For the Preceptor Table we must consider: Units, Outcome(s), Treatment, Causal or predictive model, Covariates, and Moment in Time (This is often implicit in the question itself).

### Exercise 4

What are the units for this problem?

```{r wisdom-4}
question_text(NULL,
	message = "All the men in the world, one row per man. The rows of the Preceptor Table are the units, the objects on which the outcome is measured. ",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

If the question is: What is the average height of adult males in the US, then the units are only men in the US. There is the interplay between the exact question and which units define the Preceptor Table.

### Exercise 5

What is the outcome for this problem?

```{r wisdom-5}
question_text(NULL,
	message = "Height is the outcome variable. This is not the same thing as the answer to the question we have beeen asked. But, if we can build a model which explains/understands/predicts height for an individual man, we can use that model to answer our questions.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Specifying the Preceptor Table forces us to think clearly about the units and outcomes implied by the question. The resulting discussion sometimes leads us to modify the question with which we started. No data science project follows a single direction. We always backtrack. There is always dialogue.

### Exercise 6

What are some covariates which you think might be useful for this problem, regardless of whether or not they might be included in the data?

```{r wisdom-6}
question_text(NULL,
	message = "There are no (explicit) covariates in this model, although we will need to make use of variables like age and sex to construct our sample data.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

When we are looking at a “category” of units we call this a covariate. Possible covariates include, but are not limited to, sex, age, political party and almost everything else which might be associated with our data.

### Exercise 7

What are the treatments, if any, for this problem?

```{r wisdom-7}
question_text(NULL,
	message = "There are not treatment variables.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Treatment is the thing that we are trying to see the effect of. This data has no treatment which means we cannot create a causal inference. 

### Exercise 8

What moment in time does the Preceptor Table refer to?

```{r wisdom-8}
question_text(NULL,
	message = "Now.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

This is often implicit in the question itself. One of our key roles as data scientists is to clarify the questions which we are asked. In this case, it seems clear that the questions refer to now, the present moment.

### Exercise 9

Define causal effect.

```{r wisdom-9}
question_text(NULL,
	message = "A causal effect is the difference between two potential outcomes.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Recall the example of giving individuals vitamin, the causal effect is the difference between an individual's height if he ate vitamin growing up versus not taking vitamin. Thus, we could use this same data to explore a different question which required a causal model. 

### Exercise 10

What is the fundamental problem of causal inference?

```{r wisdom-10}
question_text(NULL,
	message = "The fundamental problem of causal inference is that we can only observe one potential outcome.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

<!-- XX: More discussion about potential outcomes. Counter-factuals are hard to think about. Help students by providing discussion of the ones under consideration here, or which might be under consideration with a different question. Are they plausible? What would make them more plausible? -->
A person cannot experience both treatments at the same time. For instance, if you give a person vitamins and record their growth throughout the experiment, you cannot then rewind time, withhold the vitamins, and record their growth again under identical conditions. 

### Exercise 11

How does the motto "No causal inference without manipulation." apply in this problem?

```{r wisdom-11}
question_text(NULL,
	message = "The motto does not apply because this is a predictive, not causal, model.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

We have to choose a variable that we can change to be the treatment. If we do not have such variable that we can manipulate, then we would have to create a predictive model instead. For example, if we were focused on height, one conclusion maybe: the average height of men is expected to be higher than that of women. Correlation does not mean causation, we cannot assume that sex directly makes people taller. In order to find a causation relationship, we would need to manipulate the treatment so that we can measure its effect on the outcome.

### Exercise 12

Describe in words the Preceptor Table for this problem.

```{r wisdom-12}
question_text(NULL,
	message = "The Preceptor has two columns. There is a column for the ID, one for Height. Each row represents one individual.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The Preceptor Table for this problem looks something like this:

```{r}
#| echo: false

tibble(ID = c("Person 1", "Person 2", "...", "Person 45,000", "Person 45,001", "..."),
       height = c("150", "172", "...", "160", "142", "...")) |>
  gt() |> cols_label(ID = md("ID"), height = "Height (cm)") 
```

This is just a small portion of the actual Preceptor Table, the real one would be much larger.

Like all aspects of a data science problem, the Preceptor Table evolves as we work on the problem. For example, at the start, we aren't sure what right-hand side variables will be included in the model, so we are not yet sure which covariates must be in the Preceptor Table.

### Exercise 13

Write one sentence describing the data you have to answer your question.

```{r wisdom-13}
question_text(NULL,
	message = "The nhanes data set from the National Health and Nutrition Examination Survey conducted from 2009 to 2011 by the Centers for Disease Control and Prevention includes 15 variables, including physical attributes like weight and height.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Some variables of interest are age, sex and height. We will look at these more in depth later on. Before we continue questioning our data, let's practice creating and publishing a quarto document to answer the question we have in a professional way. Professional data scientists always store their work somewhere safe. This is so you don't lose your code even if something happens to your computer.

### Exercise 14

Create a Github repo called `two-parameters`. Make sure to click the "Add a README file" check box. 

Connect the `two-parameters` Github repo to an R project on your computer. Name the R project `two-parameters` also. 

Select `File -> New File -> Quarto Document ...`. Provide a title ("Two Parameters") and an author (you). Save the document as `analysis.qmd`. 

In the Console, run:

```
list.files(all.files = TRUE)
```

CP/CR.

```{r wisdom-14}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

The `all.files = TRUE` argument for `list.files()` generates all the files/directories, including the "hidden" ones whose names begin with a period, `.`. 

### Exercise 15

Edit the `.gitignore` by adding `*Rproj`. Save and commit this in the Git tab. Push the commit. 

In the Console, run:

```
tutorial.helpers::show_file(".gitignore", pattern = "*Rproj")
```

CP/CR.

```{r wisdom-15}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

Render the document with `Command/Control + Shift + K` to ensure everything works. This automatically saves the file as well. After ensuring everything works, remove everything below the YAML header from `analysis.qmd` and render the file again.

### Exercise 16

Let's load the important packages.

Load the **tidyverse** package.

```{r wisdom-16, exercise = TRUE}

```

```{r wisdom-16-hint-1, eval = FALSE}
library(...)
```

```{r wisdom-16-test, include = FALSE}
library(tidyverse)
```

### 

So, what does our Preceptor Table look like? Assuming we are predicting height for every adult male on Earth at this moment in time, we would have height data for every male at least 18 years of age. This means that we would have about 4 billion rows, one for each male, along with a column for each individual’s height.

### Exercise 17

Load the **primer.data** package.

```{r wisdom-17, exercise = TRUE}

```

```{r wisdom-17-hint-1, eval = FALSE}
library(...)
```

```{r wisdom-17-test, include = FALSE}
library(primer.data)
```

### 

This Preceptor Table would extend all the way until person 4 billion-and-something. If we had this table, all of our questions could be answered with simple math and/or simulations. No inference is necessary if we have a Preceptor Table.

### Exercise 18

Load the **skimr** package.

```{r wisdom-18, exercise = TRUE}

```

```{r wisdom-18-hint-1, eval = FALSE}
library(...)
```

```{r wisdom-18-test, include = FALSE}
library(skimr)
```

### 

We have loaded all the libraries in the tutorial, and we will need to do the same thing in the `analysis.qmd` except for the **skimr** package. This package is used for exploring our data which can be useful in the tutorial, but we will not be using this package in `analysis.qmd` so there is no need to load it there. 

### Exercise 19

In `analysis.qmd`, add a new code chunk and load the **tidyverse** and **primer.data** packages. Render the file.

Previously, we have added `#| echo: FALSE` to prevent repeating the code. Instead of adding this in every code chunk, we can add this in the YAML header: 

```
execute: 
  echo: false
```

To get rid of the message, add `#| message: FALSE` at the beginning of the code chunk.

Render the file again and in the Console, run: 

```
tutorial.helpers::show_file("analysis.qmd", pattern = "tidyverse|primer.data")
```

CP/CR.

```{r wisdom-19}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

The `pattern = "tidyverse|primer.data"` will only return lines that have `tidyverse` or `primer.data` in `analysis.qmd`. The operator `|` works as *or*, so that if either `tidyverse` *or* `primer.data` are present, then that line will be returned by this code.

### Exercise 20

Edit the `.gitignore` by adding `*_files`. This will cause git to ignore any files that end with "_files". We do not need these in our github repository because these are not important. 

In the Console, run:

```
tutorial.helpers::show_file(".gitignore", pattern = "*_files")
```

CP/CR.

```{r wisdom-20}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

Following the construction of the Preceptor Table is the examination of the data we have to see whether we can draw connection between these two. Several commands like `glimpse()` or `summary()` may help. 

### Exercise 21

Run `glimpse()` on `nhanes`.

```{r wisdom-21, exercise = TRUE}
    
```

```{r wisdom-21-hint-1, eval = FALSE}
glimpse(nhanes)
```

```{r wisdom-21-test, include = FALSE}
glimpse(nhanes)
```

### 

The `nhanes` data set includes 15 variables, including physical attributes like weight and height. We do not care about most of these variables for our question. 

### Exercise 22

Pipe `nhanes` to `select()` with `age`, `sex`, and `height` as parameters.

```{r wisdom-22, exercise = TRUE}
    
```

```{r wisdom-22-hint-1, eval = FALSE}
nhanes |> 
  select(...)
```

```{r wisdom-22-test, include = FALSE}
nhanes |> 
  select(age, sex, height)
```

### 

We can notice some observations about the data. A noticeable pattern is that children generally have shorter heights than adults. Again, we only care about adult men for our purpose. 

### Exercise 23

Continue the pipe and examine a random sample using `slice_sample()` and setting `n = 5`.

```{r wisdom-23, exercise = TRUE}
    
```

<button onclick="transfer_code(this)">Copy previous code</button>

```{r wisdom-23-hint-1, eval = FALSE}
... |> 
  slice_sample(...)
```

```{r wisdom-23-test, include = FALSE}
nhanes |> 
  select(age, sex, height) |> 
  slice_sample(n = 5)
```

### 

We think of both age and height as numbers. And they are numbers! But R distinguishes between “integers” and “doubles,” only the second of which allow for decimal values. In the `nhanes` data, age is an integer and height is a double.

### Exercise 24

Delete `slice_sample()` and instead use `glimpse()`.

```{r wisdom-24, exercise = TRUE}
    
```

<button onclick="transfer_code(this)">Copy previous code</button>

```{r wisdom-24-hint-1, eval = FALSE}
... |> 
  glimpse()
```

```{r wisdom-24-test, include = FALSE}
nhanes |> 
  select(age, sex, height) |> 
  glimpse()
```

### 

Be on the lookout for anything suspicious. Most data sets have some NA values, we have to get rid of these so that we can use the data. 

### Exercise 25

In addition to `glimpse()`, we can run `skim()`, from the `skimr()` package, to calculate summary statistics. Delete `glimpse()` and instead use `skim()`.

```{r wisdom-25, exercise = TRUE}
    
```

<button onclick="transfer_code(this)">Copy previous code</button>

```{r wisdom-25-hint-1, eval = FALSE}
... |> 
  skim()
```

```{r wisdom-25-test, include = FALSE}
nhanes |> 
  select(age, sex, height) |> 
  skim()
```

### 

Interesting! There are 353 missing values of height in our subset of data. Just using `glimpse()` does not show us that. Let’s filter out the NA’s using `drop_na()`. This will delete the rows in which the value of any variable is missing. 

### Exercise 26

Pipe `nhanes` to `filter()`, making sure to only have *adult men*. This means that `filter()` will have `sex` equal to `"Male"` and `age` is greater than or equal to `18`. Then, continue the pipe to `select()` to only the `height` and then finally drop the NA's using `drop_na()`.

```{r wisdom-26, exercise = TRUE}
    
```

```{r wisdom-26-hint-1, eval = FALSE}
nhanes |> 
  filter(sex == "Male", age >= 18) |> 
  select(...) |> 
  drop_na()
```

```{r wisdom-26-test, include = FALSE}
nhanes |> 
  filter(sex == "Male", age >= 18) |> 
  select(height) |> 
  drop_na()
```

### 

We have to clean the data so we can focus on the specific numbers to answer our question. Our question focuses on adult males so we have to narrow down our data to only ones that meed the requirements.

### Exercise 27

Pipe the previous code to `ggplot()`, with `height` in `x` axis in the `aes()`. Add `geom_histogram()` with `bins = 50` and then finally add `labs()` layer with the `title`, `x` and `y` axis title, and a `caption`.

```{r wisdom-27, exercise = TRUE}
    
```

<button onclick="transfer_code(this)">Copy previous code</button>

```{r wisdom-27-hint-1, eval = FALSE}
... |>
  ggplot(aes(...)) + 
    geom_histogram(...) +
    labs(...)
```

This is what the finished graph should look like:

```{r}
#| echo: false

nhanes |> 
  filter(sex == "Male", age >= 18) |> 
  select(height) |> 
  drop_na() |>
  ggplot(aes(x = height)) + 
    geom_histogram(bins = 50) +
    labs(title = "Male Adult Height in the US in 2010",
         x = "Height (cm)",
         y = "Count",
         caption = "Source: National Health and Nutrition Examination Survey"
         ) 
```

### 

Will the data we have --- which is only for a sample of adult American men more than a decade ago —-- allow us to answer our questions, however roughly? Only if the assumption of validity makes sense.

### Exercise 28

Copy your code from above and put it in a new code chunk in the `analysis.qmd` file and render the file. In the Console, run: 

````
tutorial.helpers::show_file("analysis.qmd", start = -5)
````

```{r wisdom-28}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

Eyeballing, seems like there is a 1 out of 3 chance that the next man we meet, or any randomly chosen man, is taller than 180 cm.

There are still some more questions to be answered about the data.

### Exercise 29

In your own words, define "validity" as we use the term.

```{r wisdom-29}
question_text(NULL,
	message = "Validity is the consistency, or lack thereof, in the columns of the data set and the corresponding columns in the Preceptor Table.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Validity is always about the *columns* in the Preceptor Table and the data. Just because columns from these two different tables have the same *name* does not mean that they are the same *thing*.

### Exercise 30

Provide one reason why the assumption of validity might not hold for the outcome variable: `Height`. Use the words "column" or "columns" in your answer.

```{r wisdom-30}
question_text(NULL,
	message = "Is the way people measure height in the survey similar to the way it is being measured now -- e.g. Are measurements taken with shoes on or shoes off --. Such issues can cause the `height` column in NHANES in 2010 and the column `height` in the Preceptor Table not refer to the same thing.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

There are several reasons why this data might not be valid, such as inaccurate measurements. But, for the most part, the “height” variable in NHANES in 2010 is a valid proxy for the “height” of individuals today. We can stack the two data sets together and consider them to have come from the same population.

### Exercise 31

<!-- XX: Pick the most important covariate or at least one that you know/suspect will be used in the model. This is similar to the previous question, but we want to ensure that students understand that validity is about comparing the columns in the data set with the columns in the Preceptor Table. -->

Provide one reason why the assumption of validity might not hold for the covariate: `sex`. Use the words "column" or "columns" in your answer.

```{r wisdom-31}
question_text(NULL,
	message = "The column `sex` in NHANES in 2010 and the column `sex` in the Preceptor Table may not refer to the same thing since people's notion about sex back in the 2010 might not be as diverse as now.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Because we control the Preceptor Table and, to a lesser extent, the original question, we can adjust those variables to be “closer” to the data that we actually have. This is another example of the iterative nature of data science. If the data is not close enough to the question, then we check with our boss/colleague/customer to see if we can modify the question in order to make the match between the data and the Preceptor Table close enough for validity to hold.

### Exercise 32

Summarize the state of your work so far in one or two sentences. Make reference to the data you have and to the question you are trying to answer.

```{r wisdom-32}
question_text(NULL,
	message = "Using The nhanes data set from the National Health and Nutrition Examination Survey conducted from 2009 to 2011 by the Centers for Disease Control and Prevention includes 15 variables, including physical attributes like weight and height, we seek to create a model of height for adult men.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Edit you answer as you see fit, but do not copy/paste our answer exactly. Add this summary to `analysis.qmd`, `Command/Ctrl + Shift + K`, and then commit/push.

## Justice
### 

*The arc of the moral universe is long, but it bends toward justice.* - Theodore Parker

### Exercise 1

In your own words, name the four key components of Justice for working on a data science problem.

```{r justice-1}
question_text(NULL,
	message = "Justice concerns four topics: the Population Table, stability, representativeness, and unconfoundedness.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Justice is about concerns that you (or your critics) might have, reasons why the model you create might not work as well as you hope. 

### Exercise 2

In your own words, define a Population Table.

```{r justice-2}
question_text(NULL,
	message = "The Population Table includes a row for each unit/time combination in the underlying population from which both the Preceptor Table and the data are drawn.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The Population Table is almost always much bigger than the combination of the Preceptor Table and the data because, if we can really assume that both the Preceptor Table and the data are part of the same population, than that population must cover a broad universe of time and units since the Preceptor Table and the data are, themselves, often far apart from each other.

### Exercise 3

In your own words, define the assumption of "stability" when employed in the context of data science.

```{r justice-3}
question_text(NULL,
	message = "Stability means that the relationship between the columns in the Population Table is the same for three categories of rows: the data, the Preceptor Table, and the larger population from which both are drawn.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

*The longer the time period covered by the Preceptor Table (and the data), the more suspect the assumption of stability becomes.* 

### Exercise 4

Provide one reason why the assumption of stability might not be true in this case.

```{r justice-4}
question_text(NULL,
	message = "One possible reason for the failure of the assumption of stability would be if immigrants since 2011 have a different average adult male height. This will change the overall average adult male height in America.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

A change in time or the distribution of the data does not, in and of itself, demonstrate a violation of stability. Stability is about the parameters: $\beta_1$, $\beta_1$ and so on. Stability means these parameters are the same in the data as they are in the population as they are in the Preceptor Table.

### Exercise 5

We use our data to make inferences about the overall population. We use information about the population to make inferences about the Preceptor Table: `Data -> Population -> Preceptor Table`. In your own words, define the assumption of "representativeness" when employed in the context of data science.

```{r justice-5}
question_text(NULL,
	message = "Representativeness, or the lack thereof, concerns two relationship, among the rows in the Population Table. The first is between the Preceptor Table and the other rows. The second is between our data and the other rows.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Ideally, we would like both the Preceptor Table *and* our data to be random samples from the population. Sadly, this is almost never the case.

### Exercise 6

We do not use the data, directly, to estimate missing values in the Preceptor Table. Instead, we use the data to learn about the overall population. Provide one reason, involving the relationship between the data and the population, for why the assumption of representativeness might not be true in this case.

```{r justice-6}
question_text(NULL,
	message = "Since participation in the survey is voluntary, it could mean that, for example, taller men are more likely to answer this question.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

It is important that our data is accurately reflecting the population whom we want to measure. Our data might underrepresent people who do not want to participate in the study for various reasons. If these people have a different average height compared to the overall average, then the lack of their participation will impact our data. 

### Exercise 7

We use information about the population to make inferences about the Preceptor Table. Provide one reason, involving the relationship between the population and the Preceptor Table, for why the assumption of representativeness might not be true in this case.

```{r justice-7}
question_text(NULL,
	message = "The population covers the last 20 years or more. The rows from the Preceptor Table are just for today. If the height of men today is systematically different from the height of men over the 20 year period of the Population Table, then representativeness would not be true.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Stability looks across time periods. Representativeness looks within time periods.

Of course, it is sometimes the case that the Preceptor Table includes every row from the Population Table for that moment in time. In that case, the assumption representativeness is met, by definition, if we only consider that moment. So, in that case, the only possible violation of representativeness must involve a claim that this moment in time is not representative of the rest of the Population Table.


### Exercise 8

In your own words, define the assumption of "unconfoundedness" when employed in the context of data science.

```{r justice-8}
question_text(NULL,
	message = "Unconfoundedness means that the treatment assignment is independent of the potential outcomes, when we condition on pre-treatment covariates.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

This assumption is only relevant for causal models. Our current data is not being used for a causal model so this assumption does not matter as much as the other ones. 

### Exercise 9

Summarize the state of your work so far in two or three sentences. Start with a copy from your answer from the end of the Wisdom section. Add a sentence which highlights at least one specific problem which casts doubt on your approach.

```{r justice-9}
question_text(NULL,
	message = "Using data from the National Health and Nutrition Examination Survey conducted from 2009 to 2011, we seek to create a model of height for adult men. In particular, what is the average height of an adult male in America in 2024? Since participation in the survey is voluntary, it could mean that, for example, taller men are more likely to answer this question.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Edit the summary paragraph in `analysis.qmd` as you see fit, but do not copy/paste our answer exactly. `Command/Ctrl + Shift + K`, and then commit/push.

## Courage
### 

*Courage is the commitment to begin without any guarantee of success.* - Johann Wolfgang von Goethe

### Exercise 1

In your own words, describe the components of the virtue of Courage for analyzing data.

```{r courage-1}
question_text(NULL,
	message = "Courage begins with the exploration and testing of different models. It concludes with the creation of a Data Generating Mechanism.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

In data science, we deal with words, math, and code, but the most important of these is code. We need Courage to create the model, to take the leap of faith that we can make our ideas real.

### Exercise 2

Load the **brms** package.

```{r courage-2, exercise = TRUE}

```

```{r courage-2-hint-1, eval = FALSE}
library(...)
```

```{r courage-2-test, include = FALSE}
library(brms)
```

### 

Because `height` is a continuous variable, we assume that an adult male's height is produced from a normal distribution. 

$$ height \sim Normal(\mu, \sigma^2)$$

### Exercise 3

Load the **tidybayes** package.

```{r courage-3, exercise = TRUE}

```

```{r courage-3-hint-1, eval = FALSE}
library(...)
```

```{r courage-3-test, include = FALSE}
library(tidybayes)
```

### 

Because we are using a Normal distribution, the link function is the identity. That is: 

$$ \mu =  \beta_0 + \beta_1 x_1 + \dots $$

<!-- $$ y_i =  \mu + \epsilon_i $$ -->

<!-- with $\epsilon_i \sim N(0, \sigma^2)$. $y_i$ is the height of male $i$. $\mu$ is the average height of all males in the population. $\epsilon_i$ is the "error term," the difference between the height of male $i$ and the average height of all males. $\epsilon_i$ is normally distributed with a mean of 0 and a standard deviation of $\sigma$.  -->

<!-- This is the simplest model we can construct. The model has two unknown parameters: $\mu$ and $\sigma$.  -->

### Exercise 4

Add `library(brms)` and `library(tidybayes)` to the `setup` code chunk in `analysis.qmd`. Copy and paste the below code for the mathematical structure of the model to the body of `analysis.qmd`. `Command/Ctrl + Shift + K`. 

At the Console, run:

```
tutorial.helpers::show_file("analysis.qmd", pattern = "brms|tidybayes|\\$\\$")
```

CP/CR.

```{r courage-4}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

Of course, our model must make use of the variables we actually have. Consider:

$$ \mu =  \beta_0 + \beta_1 sexMale $$
Since our covariate `sex` is a categorical variable, we cannot have words in math formula. So, a variable like sex is turned into a 0/1 variable which is then renamed as sexMale. 

### Exercise 5

Create a model using `brm()` from the **brms** package. Your arguments are `formula = height ~ 1`, `data = ch5`, `family = gaussian()`, `silent = 2`, `refresh = 0`, and `seed = 12`.

<!-- Do not forget to create this model yourself in the setup chunk. Do this once, save the object, comment out that code and then just read_rds to create the object for this tutorial. -->

<!-- Depending on code speed, you can run this function multiple times, without assigning the return value, looking at the printout, and seeing how things change. -->

<!-- AC: ch5 doesn't exist in the qmd yet. Perhaps going back to Wisdom 22 and adding something about putting it into the qmd would help. Though that would also mess up the "Copy previous code" for the next exercise.  -->

```{r courage-5, exercise = TRUE}

```

```{r courage-5-hint-1, eval = FALSE}
brm(...)
```

### 

Note:

* We will be looking at this print out many times in the rest of the *Primer*. You will get used to it. The header information about Family, Links and so on just confirms what we already know. Still, you want to check this to make sure you did not make a mistake in the call to `brm()`.

* The "Estimate" of 175.87 makes sense. After all, the simple mean of the `height` in the data is: 175.8707

```{r}
#| code-fold: false
mean(ch5$height)
```

### Exercise 6

Behind the scenes of this tutorial, an object called `fit_male_height` has been created which has the result of the `brm()` call from the code above. Type `fit_male_height` and hit "Run Code". This generates the same results as using `print(fit_male_height)`.

```{r courage-6, exercise = TRUE}

```

```{r courage-6-hint-1, eval = FALSE}
fit_male_height 
```

```{r courage-6-test, include = FALSE}
fit_male_height
```

### 

This should result in the same thing as the exercise prior to this one. 

* There is a direct connection between the mathematical form of the model created under Justice and the code we use to fit the model under Courage. `height ~ 1` is the code equivalent of $y_i =  \mu$. 

### Exercise 7

Run `family()` on `fit_male_height`. `family()` provides information about the "family" of the error term and the link between it and the dependent variable.

```{r courage-7, exercise = TRUE}

```

```{r courage-7-hint-1, eval = FALSE}
family(...)
```

```{r courage-7-test, include = FALSE}
family(fit_male_height)
```

### 

In this case, setting `family = gaussian()` implies that $\epsilon_i \sim N(0, \sigma^2)$, just as we assumed. That is not a coincidence! If $\epsilon_i$ had a different distribution, we would need to use a different statistical family. 

### Exercise 8

Run `formula()` on `fit_male_height`. `formula()` returns the statistical equation which relates the dependent variable to the independent variable(s).

```{r courage-8, exercise = TRUE}

```

```{r courage-8-hint-1, eval = FALSE}
formula(...)
```

```{r courage-8-test, include = FALSE}
formula(fit_male_height)
```

### 

In this case, the key parameter is the "Intercept," which is the same thing as $\mu$ in the mathematical description of the model and the same thing as `1` when we set `formula = height ~ 1`. In other words, we are speaking three languages here: English ("Intercept"), math ($\mu$), and code (`height ~ 1`). But all three languages are referring to the same underlying concept.

### Exercise 9

Run `nobs()` on `fit_male_height`. The `nobs()` function returns the **n**umber of **obs**ervations.

```{r courage-9, exercise = TRUE}

```

```{r courage-9-hint-1, eval = FALSE}
nobs(...)
```

```{r courage-9-test, include = FALSE}
nobs(fit_male_height)
```

### 

The output should be 3658. This is the number of observations or, in other words, it is the number of rows.

### Exercise 10

Create a new code chunk in `analysis.qmd`. Add two code chunk options: `label: model` and `cache: true`. Copy/paste the code from above for estimating the model using `brm()` into the code chunk, assigning the result to `fit_male_height`. 

`Command/Ctrl + Shift + K`. It may take some time to render `analysis.qmd`, depending on how complex your model is. But, by including `cache: true` you cause Quarto to cache the results of the chunk. The next time you render `analysis.qmd`, as long as you have not changed the code, Quarto will just load up the saved fitted object.

To confirm, `Command/Ctrl + Shift + K` again. It should be quick.

At the Console, run:

```
tutorial.helpers::show_file("analysis.qmd", start = -8)
```

CP/CR.

```{r courage-10}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

### Exercise 11

Run `posterior_interval()` on `fit_male_height`. The `posterior_interval()` function returns 95% intervals for all the parameters in our model.

```{r courage-11, exercise = TRUE}

```

```{r courage-11-hint-1, eval = FALSE}
posterior_interval(...)
```

```{r courage-11-test, include = FALSE}
posterior_interval(fit_male_height)
```

### 

### Exercise 12

Run `fixef()` on `fit_male_height`. The `fixef()` returns information about the **fix**ed **ef**fects in the model.

```{r courage-12, exercise = TRUE}

```

```{r courage-12-hint-1, eval = FALSE}
fixef(...)
```

```{r courage-12-test, include = FALSE}
fixef(fit_male_height)
```

### 

If a parameter’s estimated value is more than 2 or 3 standard errors away from zero, we generally keep that parameter (and its associated variable) in the model. This is, probably, a variable which “matters.” The main exception to this rule is a parameter whose value is so close to zero that changes in its associated variable, within the general range of that variable, can’t change the value of the outcome by much.

### Exercise 13

Write a sentence interpreting the 175.8698 estimate for Intercept.

```{r courage-13}
question_text(NULL,
	message = "Our (best) guess/estimate as to the average height of all males in the population is 178.8698 centimeters. In other words, the mean of the posterior for average height of male in the population is 178.8698 centimeters.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

$\mu$ is not the average height of the men in the sample. We can calculate that directly. It is `r mean(ch5$height)`. Instead, $\mu$ is the average height of men in the *population*. Recall that the population is the universe of people/units/whatever about which we seek to draw conclusions. On some level, this seems simple. On a deeper level, it is very subtle. Each case is a different and the details matter.

### Exercise 14

Write a sentence interpreting the ~ 0.12 standard error for the estimate.

```{r courage-14}
question_text(NULL,
	message = "The end points of our confidence interval are simply the estimate plus/minus two times the standard error (0.12).",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

$\sigma$ is an estimate for the standard deviation of the errors,  i.e., variability in height after accounting for the mean. 
Parameters like $\sigma$ in this context are *nuisance* or *auxiliary* parameters in which we still estimate their posterior distributions, but we don't really care what those posteriors look like.

### Exercise 15

Write a sentence interpreting the confidence interval for Intercept.

```{r courage-15}
question_text(NULL,
	message = "We do not know the true value for the average height of all males in the population, but we can by 95% confident that it lies somewhere between 175.63 and 176.11.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Because we are Bayesians, we believe that there is a true value and that the confidence or uncertainty interval includes it at the stated level. This is different from the Frequentists' perception that if we followed the same approach in 100 similar problems then, 95% of the time, our confidence interval would include the true value. 

### Exercise 16

Run `pp_check()` on `fit_male_height`. The `pp_check()` runs a **p**osterior **p**redictive check.

```{r courage-16, exercise = TRUE}

```

```{r courage-16-hint-1, eval = FALSE}
pp_check(...)
```

```{r courage-16-test, include = FALSE}
pp_check(fit_male_height)
```

### 

In this case, note how similar our actual data, $y$, is to the 10 versions of the replicated data, $y_rep$. They are close enough that we are happy to use `fit_male_height`. However, the match is not perfect! The actual data is slightly more "peaked" and also features some weird bumps in the tails, especially around 200 cm. It would be possible, but not easy, to1 modify our model to match the actual data more closely. For now, we will just accept `fit_male_height` as our data generating mechanism.

<!-- If the fake data had looked very different from the real data, we have had a problem. But, for the most part, we conclude that, although not perfect, pp_check() shows that the fake outcomes generated by our model are like the actual outcome data. -->

### Exercise 17

Use `library()` to load the [**gtsummary**](https://www.danieldsjoberg.com/gtsummary) package.

```{r courage-17, exercise = TRUE}

```

```{r courage-17-hint-1, eval = FALSE}
library(gtsummary)
```

```{r courage-17-test, include = FALSE}
library(gtsummary)
```

### 

We modeled `height`, a continuous variable measured in centimeters, as a linear function of a constant term. The average adult male height in the US was around 176 cm. Mathematically:

$$ height_i =  176 + \epsilon_i $$

with $\epsilon_i \sim N(0, 0.75^2)$.

<!-- The **gtsummary** package creates presentation-ready tables summarizing data sets, regression models, and more.   -->

<!-- To learn more about **gtsummary**, type `?gtsummary` in the Console, this will open up a description page. Make sure to load the **gtsummary** package in the Console first.  -->

### Exercise 18

Run `tbl_regression()` on `fit_male_height` and set `intercept` equal to `TRUE` inside `tbl_regression()`.

```{r courage-18, exercise = TRUE}

```

```{r courage-18-hint-1, eval = FALSE}
tbl_regression(...)
```

```{r courage-18-test, include = FALSE}
fit_male_height |> 
  tbl_regression(intercept = TRUE)
```

### 

This is what the table should look like:

```{r}
#| echo: false

fit_male_height |> 
  tbl_regression(intercept = TRUE)
```

We can see some important numbers from our data in this table, such as the average height of 176 cm.

### Exercise 19

Copy the code from above and put it in a new code chunk in the `analysis.qmd` file. Render the file and in the Console, run: 

```
tutorial.helpers::show_file("analysis.qmd", start = -5)
```

CP/CR.

```{r courage-19}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

The output in the `analysis.qmd` preview should have the same table from the previous question. Again, this table provides important numbers from the data we are working with. 

### Exercise 20

Write a few sentence which summarize your work so far. The first few sentences are the same as what you had at the end of the Justice section. Add at least one sentence which describes the modelling approach which you are using, specifying at least the functional form and the dependent variable. 

```{r courage-20}
question_text(NULL,
	message = "Using data from the National Health and Nutrition Examination Survey conducted from 2009 to 2011, we seek to create a model of height for adult men. In particular, what is the average height of an adult male in America in 2024? Since participation in the survey is voluntary, it could mean that, for example, taller men are more likely to answer this question. We modeled `height`, a continuous variable indicating the average height of all adult males in the population.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

### Exercise 21

Update `analysis.qmd`. First, add `library(gtsummary)` to the `setup` code chunk,. Second, add the mathematical formula, in $\LaTeX$ and surrounded by double dollar signs, for your model. Third, add a new code chunk which creates the table of model parameters. `Command/Ctrl + Shift + K` to ensure that everything works.

At the Console, run:

```
tutorial.helpers::show_file("analysis.qmd", start = -8)
```

CP/CR.

```{r courage-21}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

## Temperance
### 

*Temperance is a tree which as for its root very little contentment, and for its fruit calm and peace.* - Buddha 

### Exercise 1

In your own words, describe the use of Temperance in data science.

```{r temperance-1}
question_text(NULL,
	message = 'Temperance uses the Data Generating Mechanism to answer the question with which we began. Humility reminds us that this answer is always a lie.',
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The Data Generating Mechanism is our model of the world. If you are a genius, you can just look at the math and know what it means. Normal people need more. They need you to display, graphically, what the model means. This graphic display will almost always be a collection of posteriors.

### Exercise 2

What is the general topic we are investigating? What is the specific question we are trying to answer? 

```{r temperance-2}
question_text(NULL,
	message = "We are investigating the height for adult men to answer the question: What is the probability that the next man we meet will be taller than 180 centimeters?",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Data science projects almost always begin with a broad topic of interest. Yet, in order to make progress, we need to drill down to a specific question. This leads to the creation of a data generating mechanism, which can now be used to answer lots of questions, thus allowing us to explore the original topic broadly.

### Exercise 3

To answer our question, we need to create a `newdata` object. Which variables (e.g., which columns) do we need to include in this object?

```{r temperance-3}
question_text(NULL,
	message = "Our model is so simple that there are no right-hand side variables, other than the intercept. In most statistical models, the intercept is assumed to be there, so we do not need to include it. So, there are no columns in our newdata object.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

We should be modest in the claims we make. The posteriors we create are never the “truth.” The assumptions we made to create the model are never perfect. Yet decisions made with flawed posteriors are almost always better than decisions made without them.

### Exercise 4

Which values do you want the variables in your `newdata` object to have? This is not easy! 

```{r temperance-4}
question_text(NULL,
	message = "The newdata object has no variables, but it does need t to have one (empty) row. This will allow us to generate one posterior for average/expected height.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

### Exercise 5

Here is the R code which creates the `newdata` object: `tibble(.rows = 1)`. Type it into the code exercise block and hit "Run Code."


```{r temperance-5, exercise = TRUE}

```

```{r temperance-5-hint-1, eval = FALSE}
tibble(...)
```

```{r temperance-5-test, include = FALSE}
tibble(.rows = 1)
```

### 

`tibble()` is used to create a data frame. The `.rows = 1` creates a tibble that only has one row. 

### Exercise 6

Behind the scenes, we have created the `ndata` object using this code. Type `ndata` and hit "Run Code." 

```{r temperance-6, exercise = TRUE}

```

```{r temperance-6-hint-1, eval = FALSE}
ndata
```

```{r temperance-6-test, include = FALSE}
ndata
```

### 

Note that, when you add Temperance-related code to your QMD, you will need to also add code which creates the `ndata` object.

### Exercise 7

Now that we have the `ndata` object, we can create a pipe which uses out fitted model to answer our question. Begin by typing `fit_male_height` and clicking "Run Code."

```{r temperance-7, exercise = TRUE}

```

```{r temperance-7-test, include = FALSE}
fit_male_height
```

### 

We use `add_epred_draws()` for expected values: we are not interested in any specific individual, the individual value is irrelevant. 

We use `add_predicted_draws()` for predicted values: we care, not about the average, but about this specific person.

Both functions return draws from a posterior probability distribution, but the unknown number which underlies the posterior are very different.

### Exercise 8

Pipe `fit_male_height` to `add_predicted_draws()` with the argument `newdata = ndata`.

```{r temperance-8, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r temperance-8-hint-1, eval = FALSE}
... |> 
  add_predicted_draws(...)
```

```{r temperance-8-test, include = FALSE}
fit_male_height |> 
  add_predicted_draws(newdata = ndata)
```

### 

The vector of data is 4,000 draws from the posterior probability distribution for the average height of an adult male. 

### Exercise 9

Continue the pipe to `ggplot()` with `x` set to `.prediction` inside `aes()`. Then add `geom_histogram()` with `y` set to `after_stat(count / sum(count))` inside `aes()`. Also include `bins = 100` inside `geom_histogram()`.

```{r temperance-9, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r temperance-9-hint-1, eval = FALSE}
... |> 
  ggplot(aes(x = ...)) +
    geom_histogram(aes(y = ...), ...)
```

```{r temperance-9-test, include = FALSE}
fit_male_height |> 
  add_predicted_draws(newdata = ndata) |> 
  ggplot(aes(x = .prediction)) +
    geom_histogram(aes(y = after_stat(count / sum(count))),
                   bins = 100)
```

### 

The graph is starting to take form. We can see the shape of the graph. It is a bell curve with a center at roughly 176 cm.

### Exercise 10

Add a layer of `labs()` with a `title`, `subtitle`, `x`, `y`, and `caption`.

```{r temperance-10, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r temperance-10-hint-1, eval = FALSE}
... +
    labs(...)
```

```{r temperance-10-test, include = FALSE}
fit_male_height |> 
  add_predicted_draws(newdata = ndata) |> 
  ggplot(aes(x = .prediction)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100) +
    labs(title = "Posterior for Height of Random Male",
         subtitle = "Uncertainty for a single individual is much greater than for the expected value",
         x = "Height (cm)",
         y = "Probability",
         caption = "Data source: NHANES")
```

### 

At this point, the graph should look like this: 

```{r}
#| echo: false

fit_male_height |> 
  add_predicted_draws(newdata = ndata) |> 
  ggplot(aes(x = .prediction)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100) +
    labs(title = "Posterior for Height of Random Male",
         subtitle = "Uncertainty for a single individual is much greater than for the expected value",
         x = "Height (cm)",
         y = "Probability",
         caption = "Data source: NHANES")
```

### Exercise 11

Finally, add `scale_x_continuous()` and `scale_y_continuous()`. Inside `scale_x_continuous()`, have `labels = scales::number_format()` and inside `scale_y_continuous()`, have `labels = scales::percent_format(accuracy = 1)`.

```{r temperance-11, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r temperance-11-hint-1, eval = FALSE}
... + 
    scale_x_continuous(...) +
    scale_y_continuous(...)
```

```{r temperance-11-test, include = FALSE}
fit_male_height |> 
  add_predicted_draws(newdata = ndata) |> 
  ggplot(aes(x = .prediction)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100) +
    labs(title = "Posterior for Height of Random Male",
         subtitle = "Uncertainty for a single individual is much greater than for the expected value",
         x = "Height (cm)",
         y = "Probability",
         caption = "Data source: NHANES") + 
    scale_x_continuous(labels = scales::number_format()) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1))
```

### 

The posterior for an individual is much wider than the posterior for the expected value.

Eyeballing, seems like there is a 1 out of 3 chance that the next man we meet, or any randomly chosen man, is taller than 180 cm.

### Exercise 12

Create a new code chunk in `analysis.qmd`. Label it with `#| label: plot`. Copy/paste the code which creates your graphic.  

You will need to create your own `ndata` object in `analysis.qmd` to make this code work. The code to make `ndata` is `ndata <- tibble(.rows = 1)`

`Command/Ctrl + Shift + K` to ensure that it all works as intended. At the Console, run:

```
tutorial.helpers::show_file("analysis.qmd", start = -8)
```

CP/CR.

```{r temperance-12}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

We can calculate the exact probability by manipulating the tibble of draws directly.

### Exercise 13

<!-- AC: Again, might be better to set the seed here as well.  -->

Start a new pipe with `fit_male_height` to `add_predicted_draws()` with the argument `newdata = ndata`.

```{r temperance-13, exercise = TRUE}

```

```{r temperance-13-hint-1, eval = FALSE}
... |> 
  add_predicted_draws(...)
```

```{r temperance-13-test, include = FALSE}
fit_male_height |> 
  add_predicted_draws(newdata = ndata)
```

### 

This time, we will not make a graph. Instead, we will be finding the exact answer to our question by changing the data. 

### Exercise 14

Continue the pipe to `mutate()`. Inside, have `tall` set to `if_else()`. Have parameters `.prediction > 180, TRUE, FALSE` inside `if_else()`.

```{r temperance-14, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r temperance-14-hint-1, eval = FALSE}
... |> 
  mutate(...)
```

```{r temperance-14-test, include = FALSE}
fit_male_height |> 
  add_predicted_draws(newdata = tibble(.rows = 1)) |> 
  mutate(tall = if_else(.prediction > 180, TRUE, FALSE))
```

### 

Again, the key conceptual difficulty is the population. Now that we have created a model, we look to the virtue of Temperance for guidance in using that model. The data we have is never a perfect match for the world we face. We need to temper our confidence and act with humility. Our forecasts will never be as good as a naive use of the model might suggest. Reality will surprise us. We need to take the model’s claims with a family-sized portion of salt.

### Exercise 15

Finish up the pipe with `summarize()`. Inside, have `odds` set to `mean()` with parameter `tall`.

```{r temperance-15, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r temperance-15-hint-1, eval = FALSE}
... |> 
  summarize(...)
```

```{r temperance-15-test, include = FALSE}
fit_male_height |> 
  add_predicted_draws(newdata = tibble(.rows = 1)) |> 
  mutate(tall = if_else(.prediction > 180, TRUE, FALSE)) |>
  summarize(odds = mean(tall))
```

### 

If 30% or so of the draws from the posterior probability distribution are greater than 180 cm, then there is about a 30% chance that the next individual will be taller than 180 cm.

### Exercise 16

Copy/paste the code which creates the answer into a new code chunk in `analysis.qmd`. `Command/Ctrl + Shift + K` to ensure that it all works as intended.

At the Console, run:

```
tutorial.helpers::show_file("analysis.qmd", start = -8)
```

CP/CR.

```{r temperance-16}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

We have answered our specific question. But the fun does not stop here! Just because one question was answered does not mean that we can’t consider other questions. Real data scientists look at many related questions to answer, not just one. But for this tutorial, we will only look at this one question. 

### Exercise 17

Write a paragraph which summarizes the project in your own words. The first few sentences are the same as what you had at the end of the Courage section. But, since your question may have evolved, you should feel free to change those sentences. Add at least one sentence which describes at least one quantity of interest (QoI) --- presumably one that answers your question -- and which provides a measure of uncertainty about that QoI.

```{r temperance-17}
question_text(NULL,
	message = "Using data from the National Health and Nutrition Examination Survey conducted from 2009 to 2011, we seek to create a model of height for adult men. In particular, what is the average height of an adult male in America in 2024? Since participation in the survey is voluntary, it could mean that, for example, taller men are more likely to answer this question. We modeled `height`, a continuous variable indicating the average height of all adult males in the population. The average height of an adult men in America in 2024 is 176 cm, with standard deviation of 7.48 cm.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Revise your answer to make it better and then add it to the `analysis.qmd` file. Do not just copy/paste our answer.

As always, note that your estimate depends on all the assumptions in your model being true. But that is a lie! All your assumptions are not true. So, your mean estimate might be wrong, and your confidence intervals are certainly too narrow.

### Exercise 18

Rearrange the material in `analysis.qmd` so that the order is graphic, paragraph, math and table. Doing so, of course, requires sensible judgment. For example, the code chunk which creates the fitted model must occur before the chunk which creates the graphic. `Command/Ctrl + Shift + K` to ensure that everything works.

At the Console, run:

```
tutorial.helpers::show_file("analysis.qmd")
```

CP/CR.

```{r temperance-18}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

Add `rsconnect` to the `.gitignore` file. You don't want your personal Rpubs details stored in the clear on Github. Commit/push everything.

### Exercise 19

Publish the `two-parameters` quarto document to Rpubs. Copy and paste the link of the published document.

```{r temperance-19}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

## Summary
### 

This tutorial covered [Chapter 5: Two Parameters](https://ppbds.github.io/primer/two-parameters.html) of [*Preceptor’s Primer for Bayesian Data Science: Using the Cardinal Virtues for Inference*](https://ppbds.github.io/primer/) by [David Kane](https://davidkane.info/).

```{r download-answers, child = system.file("child_documents/download_answers.Rmd", package = "tutorial.helpers")}
```
